<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.4"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>EasyMVS: Workflow and Pipeline</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">EasyMVS
   </div>
   <div id="projectbrief">simple 3D reconstruction framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.4 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">Workflow and Pipeline </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="autotoc_md24"></a>
Module function:</h1>
<h2><a class="anchor" id="autotoc_md25"></a>
Camera initialization:</h2>
<p >Import internal reference</p>
<p >import</p>
<p >Initialize Sensors/Initialize Filesystem</p>
<h3><a class="anchor" id="autotoc_md26"></a>
Enter:</h3>
<p >File Address/Sensor Device ID</p>
<h3><a class="anchor" id="autotoc_md27"></a>
Output:</h3>
<p >none</p>
<h3><a class="anchor" id="autotoc_md28"></a>
Signal:</h3>
<p >Device opened successfully/failed to open</p>
<h3><a class="anchor" id="autotoc_md29"></a>
illustrate:</h3>
<p >Model the multi-eye camera as a camera group composed of multiple monocular cameras, so that it is convenient to unify the monocular model and the binocular model</p>
<p >The camera here needs to distinguish between the slave camera and the main camera, and distinguish between the matching feature point camera and the unmatchable feature point camera</p>
<h2><a class="anchor" id="autotoc_md30"></a>
Load image:</h2>
<p >The main function is to take pictures</p>
<p >Binocular camera: take pictures (synchronous exposure, lighting) and remove distortion</p>
<p >RGBD camera: Take multiple photos, get a depth map, and remove distortion</p>
<p >Monocular camera/picture: take pictures/read pictures</p>
<h3><a class="anchor" id="autotoc_md31"></a>
Enter:</h3>
<p >Files/Camera/Projector</p>
<h3><a class="anchor" id="autotoc_md32"></a>
Output:</h3>
<p >De-distort image/depth map/shooting time stamp</p>
<h3><a class="anchor" id="autotoc_md33"></a>
Signal:</h3>
<p >A frame shooting completion signal</p>
<h3><a class="anchor" id="autotoc_md34"></a>
illustrate:</h3>
<p >The binocular camera is triggered synchronously, and the RGBD camera needs to take multiple sets of pictures, and the output here is to obtain the depth map and RGB map (the process can be subdivided into threads to realize the pipeline).</p>
<h2><a class="anchor" id="autotoc_md35"></a>
Feature extraction:</h2>
<p >Extract image features</p>
<p >SURF/SIFT/ORB features</p>
<p >custom mark point</p>
<h3><a class="anchor" id="autotoc_md36"></a>
Enter:</h3>
<p >Images to be matched</p>
<p >matching parameters</p>
<h3><a class="anchor" id="autotoc_md37"></a>
Output:</h3>
<p >feature point set</p>
<h3><a class="anchor" id="autotoc_md38"></a>
illustrate:</h3>
<p >You can use OpenCV's feature descriptor to process, pay attention to agreeing with your own mark point descriptor and OpenCV's descriptor</p>
<p >Pay attention to the situation where no feature points can be found</p>
<h2><a class="anchor" id="autotoc_md39"></a>
Feature matching:</h2>
<p >Match image features</p>
<h3><a class="anchor" id="autotoc_md40"></a>
Enter:</h3>
<p >Feature point set, codebook (optional) (may be required for bag-of-words model)</p>
<h3><a class="anchor" id="autotoc_md41"></a>
Output:</h3>
<p >Feature vector/matching result (successful matching result/no matching result)</p>
<p >Codebook (for subsequent loopback detection, etc.)</p>
<h3><a class="anchor" id="autotoc_md42"></a>
illustrate:</h3>
<p >Matching methods include brute force matching, preselected camera pair + word bag tree, sequential matching</p>
<h2><a class="anchor" id="autotoc_md43"></a>
Posture recovery:</h2>
<p >Restoring the posture according to the feature point matching relationship,</p>
<p >Restoring the attitude according to the preset camera position (binocular extrinsic parameters)</p>
<p >Restoring pose based on external input (high-precision robotic arm)</p>
<p >Restoring posture based on external input and image relationship (car/low-precision robotic arm/VIO)</p>
<h3><a class="anchor" id="autotoc_md44"></a>
Enter:</h3>
<p >Feature point matching result</p>
<p >prior pose information</p>
<p >preset camera pair</p>
<h3><a class="anchor" id="autotoc_md45"></a>
Output:</h3>
<p >attitude</p>
<h3><a class="anchor" id="autotoc_md46"></a>
illustrate:</h3>
<p >There should be a small map class here</p>
<p >Find initial camera pair -&gt; calculate initial sparse 3D points -&gt; select next camera (sequential input/best match) -&gt; pnp and calculate relative pose -&gt; calculate new sparse 3D points based on matching -&gt; local BA optimization (with motion equation fusion)</p>
<h2><a class="anchor" id="autotoc_md47"></a>
Loop detection/posture optimization:</h2>
<p >Calculate loop closure based on feature points and preset dictionary (optional)</p>
<p >Perform closed-loop optimization/graph optimization/BA optimization</p>
<h3><a class="anchor" id="autotoc_md48"></a>
Enter:</h3>
<p >feature point set/pose</p>
<h3><a class="anchor" id="autotoc_md49"></a>
Output:</h3>
<p >attitude</p>
<h3><a class="anchor" id="autotoc_md50"></a>
illustrate:</h3>
<p >Here is a big map class</p>
<p >(How to do global optimization is to be determined)</p>
<h2><a class="anchor" id="autotoc_md51"></a>
Dense Mapping:</h2>
<p >Generate dense point clouds, using methods such as SGM, SSD, SAD or self-developed methods</p>
<p >Binocular Camera: Binocular Dense Reconstruction</p>
<p >Structured light camera: directly output dense results/output filtered results</p>
<p >Monocular camera: select the appropriate camera pair, and binocular dense reconstruction</p>
<h3><a class="anchor" id="autotoc_md52"></a>
Enter:</h3>
<p >Feature points/feature point matching/camera pose/optimal camera pair (optional)</p>
<p >Image Pair/Depth Map</p>
<h3><a class="anchor" id="autotoc_md53"></a>
Output:</h3>
<p >Depth map/point cloud</p>
<h3><a class="anchor" id="autotoc_md54"></a>
illustrate:</h3>
<p >Match the optimal camera pair -&gt; binocular/multi-eye vision to generate a depth map (use a depth map instead of a point cloud to facilitate subsequent fusion)</p>
<p >The dense matching algorithm can use existing algorithms or use self-developed algorithms</p>
<h2><a class="anchor" id="autotoc_md55"></a>
Global optimization:</h2>
<p >Multi-piece point cloud splicing and fusion</p>
<p >ICP/SDF/TSDF/Poisson sampling/NeRF(ML)</p>
<h3><a class="anchor" id="autotoc_md56"></a>
Enter:</h3>
<p >Depth map/point cloud</p>
<p >attitude</p>
<h3><a class="anchor" id="autotoc_md57"></a>
Output:</h3>
<p >Filtered point cloud/voxel/octree/implicit expression</p>
<h3><a class="anchor" id="autotoc_md58"></a>
illustrate:</h3>
<p >If you use NeRF, you can skip the dense mapping part</p>
<h2><a class="anchor" id="autotoc_md59"></a>
Point cloud processing:</h2>
<p >filtering</p>
<h3><a class="anchor" id="autotoc_md60"></a>
Enter:</h3>
<h3><a class="anchor" id="autotoc_md61"></a>
Output:</h3>
<h3><a class="anchor" id="autotoc_md62"></a>
Signal:</h3>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.4
</small></address>
</body>
</html>
